## llm-dev-agent

Minimal, cost-efficient Dev Agent that can autonomously edit code in a project with a Spring Boot (Java) backend and a Vue.js frontend. It indexes your repo, retrieves relevant context, asks an LLM to generate a unified git diff, applies the patch, creates a branch, commits, pushes, and opens a GitHub Pull Request. Supports local models via Ollama as the default (low cost), and OpenAI as an alternative.

### Features
- Indexes code with ChromaDB for semantic search (.java, .vue, .js, .html by default)
- Local-first LLM via Ollama (e.g., `mistral`, `codellama`, `codegemma`) or OpenAI (`gpt-4o-mini`)
- Generates a unified git diff (no explanations) and saves it to `generated.patch`
- Applies patch, creates `feat/<slug>` branch, commits, pushes, and opens a PR
- Task input from manual prompt or Trello (with sufficiency analysis)

### Requirements
- Python 3.10+
- Git installed and a local repo with `origin` remote
- One of:
  - Ollama running locally with desired model(s)
  - OpenAI API Key
- GitHub Personal Access Token (repo scope) for PR creation

### Install
```bash
pip3 install openai==1.3.0 PyGithub chromadb pyyaml requests
pip3 install pysqlite3-binary
```

## If using Ollama (recommended for low cost):

### Installation:

**macOS:**
```bash
brew install ollama  # or download from https://ollama.com/download
```

**Linux:**
```bash
curl -fsSL https://ollama.com/install.sh | sh 
```

**Windows**
```bash
Download installer from https://ollama.com/download/windows
```

### Pull required models:
```bash
ollama pull mistral
ollama pull nomic-embed-text # for embedding (vectors)
```

### Start Ollama service:

***macOS/Linux:***
```bash
ollama serve  # runs on http://localhost:11434
```
***Windows:***
- Ollama runs automatically after installation

### Alternative models you can use:
```bash
ollama pull llama2        # Meta's Llama 2
ollama pull codellama     # Code-focused model
ollama pull phi           # Microsoft's small model
ollama pull mixtral       # Larger, more capable model
```

### Check if Ollama is running:
```bash
curl http://localhost:11434/api/tags
```
### Configuration
Edit `llm-dev-agent/config.yaml`:
```yaml
project_path: "/absolute/path/to/your/monorepo"

# LLM provider: ollama | openai
provider: "ollama"
model: "mistral:latest"
embeddings_model: "nomic-embed-text:latest"

openai_api_key: ""
openai_model: "gpt-4o-mini"
openai_embeddings_model: "text-embedding-3-small"

ollama:
  host: "http://localhost:11434"

github:
  token: "YOUR_GITHUB_TOKEN"
  repo: "owner/repo"
  base_branch: "main"
  pr_title_prefix: "feat:"
  pr_body_footer: "\n\nGenerated by llm-dev-agent"

index:
  include_extensions: [".java", ".vue", ".js", ".html", ".yaml", ".yml"]
  exclude_dirs: ["node_modules", "target", "dist", ".git", "build", "monitoring"]

agent:
  top_k: 8
  output_patch_path: "generated"
  dry_run: false
  task_source: "manual"  # manual | trello

trello:
  enabled: false
  api_key: ""
  token: ""
  board_id: ""   # optional if list_id or card_id provided
  list_id: ""    # optional if board_id or card_id provided
  card_id: ""    # optional; if set, picks this card
```

Notes:
- For OpenAI, set `provider: openai` and fill `openai_api_key`.
- For Ollama, keep `provider: ollama` and ensure the model and embeddings are pulled locally.
- Set `project_path` to your repository root (the place with `.git/`).
- Ensure `github.repo` matches `origin` remote.

### Usage
Run the agent:
```bash
python3 main.py
```

Flow:
1) The agent loads config and builds/refreshes the code index.
2) It gets a task description from the selected source:
   - Manual: prompts you to paste a description
   - Trello: fetches a card from `card_id` or the first card from `list_id`/`board_id`
3) If Trello is used, it analyzes if the card has sufficient details and prints any missing info.
4) Sends task + relevant snippets to the LLM to produce a unified git diff.
5) Saves the diff to `generated.patch` and asks to apply it.
6) On approval, creates a `feat/<short-task-name>` branch, applies, commits, pushes, and opens a PR.

### Trello Task Source
Enable in `config.yaml`:
```yaml
agent:
  task_source: "trello"
trello:
  enabled: true
  api_key: "YOUR_TRELLO_KEY"
  token: "YOUR_TRELLO_TOKEN"
  board_id: "optional"
  list_id: "optional"
  card_id: "optional"
```

Behavior:
- If `card_id` is set: uses that specific card
- Else if `list_id` is set: uses the first card in the list
- Else if `board_id` is set: uses the first card on the board
- Prints a sufficiency verdict with missing items (if any) before generating a diff

### Internals Overview
- `main.py`: Orchestrates config, indexing, task retrieval, analysis, diff generation, and PR flow
- `modules/config_loader.py`: Typed config handling
- `modules/code_indexer.py`: ChromaDB indexing; embeddings via Ollama/OpenAI
- `modules/llm_agent.py`: Produces a unified git diff (no prose)
- `modules/task_source.py`: Manual or Trello task providers
- `modules/task_analyzer.py`: LLM-based sufficiency check (JSON verdict)
- `modules/patch_applier.py`: Branch, patch apply, commit, push, PR
- `modules/github_client.py`: PyGithub wrapper

### Tips
- Keep tasks specific (files, components, routes, endpoints) to improve diff quality
- Adjust `agent.top_k` if retrieval is too narrow/wide
- You can run with `dry_run: true` to skip applying/PR creation and just inspect `generated.patch`

### Troubleshooting
- Ollama connection error: ensure `ollama serve` is running and `ollama.host` is correct
- Missing GitHub permissions: verify token has `repo` scope and remote `origin` is set
- Patch apply failures: open `generated.patch`, check paths and context; re-run with a more specific task
- Empty or low-quality diffs: increase detail in task description, raise `top_k`, or switch provider/model


